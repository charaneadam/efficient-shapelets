\documentclass[sigconf, nonacm]{acmart}

\begin{document}
\title{Efficient Selection of Time Series Shapelets}

\author{Adam Charane}
\affiliation{%
	\institution{Free University of Bozen-Bolzano}
	\city{Bolzano}
	\state{Italy}
}
\email{acharane@unibz.it}

\author{Matteo Ceccarello}
\affiliation{%
	\institution{University of Padova}
	\city{Padova}
	\state{Italy}
}
\email{matteo.ceccarello@dei.unipd.it}

\author{Johann Gamper}
\affiliation{%
	\institution{Free University of Bozen-Bolzano}
	\city{Bolzano}
	\state{Italy}
}
\email{johann.gamper@unibz.it}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
	Praesent imperdiet, lacus nec varius placerat, est ex eleifend justo, a vulputate leo massa consectetur nunc. Donec posuere in mi ut tempus. Pellentesque sem odio, faucibus non mi in, laoreet maximus arcu. In hac habitasse platea dictumst. Nunc euismod neque eu urna accumsan, vitae vehicula metus tincidunt. Maecenas congue tortor nec varius pellentesque. Pellentesque bibendum libero ac dignissim euismod. Aliquam justo ante, pretium vel mollis sed, consectetur accumsan nibh. Nulla sit amet sollicitudin est. Etiam ullamcorper diam a sapien lacinia faucibus.
\end{abstract}

\maketitle

\section{Introduction}
Time series classification is similar to traditional classification where the
difference is that the order of the values of the time series is crucial.
The order holds discriminatory features for the time series. The literature
is rich of algorithms for classification, and one way to group these
algorithms is based on the techniques that each algorithm uses to exploit the
discriminatory features suggested by Bagnall et al.~\cite{bake_off}:
\begin{itemize}
	\item Distance based: Classification is based on using distance measures to
	      the raw time series.
	\item Feature based: Global features are extracted and the data is transformed
	      from the time domain to the feature domain spanned by the extracted
	      features, and the classification is done using a standard classifier.
	\item Interval based: Techniques in this family extract one or more intervals,
	      then statistical summaries of these intervals are used as features.
	\item Shapelet based: This technique exploits discriminatory patterns that
	      appear in time series of the same class.
	\item Dictionary based: These algorithms use as features the count of
	      repeating patterns.
	\item Convolution based: Features used are extracted from time series using
	      convolutions and pooling operations.
	\item Deep learning: Classification is done by feeding the data to neural
	      networks
	\item Hybrid: Algorithms in this category combine two or more approaches from
	      the previous techniques.
\end{itemize}
Our focus in this paper is on extracting discriminating patterns that represent
time series from each class. The extracted patterns then can be used by
algorithms from the shapelet based technique in order to execute the
classification task.

Time series classification using shapelets is usually done over three phases:
\begin{enumerate}
	\item Extraction of candidates~\cite{keogh_shapelet, random_shapelets, fss}:
	      Select subsequences that can be used as discriminating patterns.
	\item Evaluation of the candidates~\cite{alternative_measures,
		      shapelet_transform, silhoutte_shapelets}: Compute some statistics
	      about the extracted patterns in order to rank and select the best
	      ones (the most discriminating).
	\item Data Transformation~\cite{shapelet_transform}: After the best
	      candidates are chosen, the time series have to be transformed to a 
        format that can be used by standard classifiers.
\end{enumerate}
The extraction step can be done in different ways such as considering all the
subsequences. However, this results in high number of candidates. Namely, given
a dataset of $m$ time series each having a length $n$, then the number of
candidates considered is in the order of $\mathcal{O}(n^2m)$. Another popular
alternative is to randomly select subsequences from different time series and
from different positions. Another new approach is based on selecting
subsequences based on some statistics that can be computed efficiently such as
rolling mean, variance, change in the slope or other simple statistics.

Extracting candidates by itself can be done efficiently, but the challenge is
the second step, as the evaluation is based on the similarity between the
candidates, or between the candidates and the time series. For instance, if the
candidates are extracted using brute force(considering all the subsequences)
then computing evaluation statistics would require computing distances between
all pairs which is in the order of $O(n^4m^2)$. Clearly, this is very
inefficient and becomes impossible for even relatively small datasets.

Instead of evaluating by giving a score to each candidate independently, our
approach use efficient similarity search indexing approaches and exploits the
similarity structure in order to select the best candidates. The idea is that
good shapelets representing a class $C$ should have a lot nearest neighbors
that are from the same class, and at the same time, should have few or no
neighbors from other classes.

\section{Related work}
NN-Descent~\cite{NN-Descent} is based on the principle: \textit{A neighbor of
	a neighbor is likely to be a neighbor}. The idea is, given an approximation
graph of the nearest neighbor, then the approximation can be improved,
in an iterative fashion, by exploring the neighbors of the neighbors of each
point. To improve the running time, the authors use local join to avoid
comparing same points many times in the same iteration. They also use
incremental search to avoid comparing points that have been compared in
previous iterations. Also, sampled is used to lower the overhead of local
joins. Finally, random projection trees are used to initialize the graph.

\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}

\end{document}
\endinput
